# ═══════════════════════════════════════════════════════════════════════════════
# JARVIS Agent Configuration
# ═══════════════════════════════════════════════════════════════════════════════

# ─────────────────────────────────────────────────────────────────────────────────
# VARIANT SELECTION
# Options: "productivity" | "balanced"
# - productivity: Single-agent, low resource, conservative approval
# - balanced: Multi-agent routing, high resource, balanced approval
# ─────────────────────────────────────────────────────────────────────────────────
JARVIS_VARIANT=balanced

# ─────────────────────────────────────────────────────────────────────────────────
# LLM PROVIDER API KEYS
# ─────────────────────────────────────────────────────────────────────────────────

# Primary: Anthropic Claude (Claude 3.5 Sonnet)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Fallback 1: OpenAI GPT-4o
OPENAI_API_KEY=your_openai_api_key_here

# Fallback 2: Google Gemini (optional)
GOOGLE_AI_API_KEY=your_google_ai_api_key_here

# ─────────────────────────────────────────────────────────────────────────────────
# LOCAL MODEL CONFIGURATION (Ollama)
# Used for offline/privacy mode
# ─────────────────────────────────────────────────────────────────────────────────

# Ollama server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Default model based on variant:
# - productivity: llama3:8b or mistral:7b (8-16GB RAM)
# - balanced: llama3:70b (48GB+ RAM)
OLLAMA_MODEL=llama3:8b

# ─────────────────────────────────────────────────────────────────────────────────
# PROVIDER PRIORITY
# Comma-separated list of providers in fallback order
# Options: anthropic, openai, gemini, ollama
# ─────────────────────────────────────────────────────────────────────────────────
LLM_PROVIDER_PRIORITY=anthropic,openai,ollama

# ─────────────────────────────────────────────────────────────────────────────────
# AGENT CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────────────

# Maximum tokens for context window (default: 100000 for Claude)
MAX_CONTEXT_TOKENS=100000

# Maximum tokens for agent response
MAX_RESPONSE_TOKENS=4096

# Temperature for LLM responses (0.0 - 1.0)
LLM_TEMPERATURE=0.7

# ─────────────────────────────────────────────────────────────────────────────────
# TOOL APPROVAL SETTINGS
# ─────────────────────────────────────────────────────────────────────────────────

# Approval mode based on variant:
# - conservative: All dangerous tools require approval
# - balanced: Only file deletion/system commands require approval
# - trust: No approval required (NOT recommended for production)
TOOL_APPROVAL_MODE=balanced

# ─────────────────────────────────────────────────────────────────────────────────
# MEMORY & PERSISTENCE
# ─────────────────────────────────────────────────────────────────────────────────

# Path to persistent memory file
MEMORY_FILE_PATH=./memory/memory.md

# Session compaction threshold (number of messages before compaction)
SESSION_COMPACTION_THRESHOLD=50

# ─────────────────────────────────────────────────────────────────────────────────
# LOGGING
# ─────────────────────────────────────────────────────────────────────────────────

# Log level: error, warn, info, debug
LOG_LEVEL=info

# Log output: console, file, both
LOG_OUTPUT=both

# Log file path
LOG_FILE_PATH=./logs/jarvis.log
